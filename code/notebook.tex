
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{ctexart}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{12.topic-models-with-turicreate}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{topic-modeling-using-graphlab}{%
\section{Topic Modeling Using
Graphlab}\label{topic-modeling-using-graphlab}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

王成军

wangchengjun@nju.edu.cn

计算传播网 http://computational-communication.com

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{turicreate} \PY{k}{as} \PY{n+nn}{tc}
\end{Verbatim}


    Download Data:
http://select.cs.cmu.edu/code/graphlab/datasets/wikipedia/wikipedia\_raw/w15

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{sf} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{SFrame}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/Users/datalab/bigdata/cjc/w15}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                                      \PY{n}{header}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Finished parsing file /Users/datalab/bigdata/cjc/w15
    \end{verbatim}

    
    
    \begin{verbatim}
Parsing completed. Parsed 100 lines in 0.447868 secs.
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
------------------------------------------------------
Inferred types from first 100 line(s) of file as 
column\_type\_hints=[str]
If parsing fails due to incorrect types, you can correct
the inferred type list above and pass it to read\_csv in
the column\_type\_hints argument
------------------------------------------------------

    \end{Verbatim}

    
    \begin{verbatim}
Read 12278 lines. Lines per second: 16914.5
    \end{verbatim}

    
    
    \begin{verbatim}
Finished parsing file /Users/datalab/bigdata/cjc/w15
    \end{verbatim}

    
    
    \begin{verbatim}
Parsing completed. Parsed 72269 lines in 1.73223 secs.
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{sf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} Columns:
        	X1	str
        
        Rows: 72269
        
        Data:
        +-------------------------------+
        |               X1              |
        +-------------------------------+
        | aynrand born and educated {\ldots} |
        | asphalt in american englis{\ldots} |
        | actinopterygii the actinop{\ldots} |
        | altaiclanguages these lang{\ldots} |
        | argon the name argon is de{\ldots} |
        | augustderleth a 1938 gugge{\ldots} |
        | amateur amateurism can be {\ldots} |
        | assemblyline an assembly l{\ldots} |
        | astronomicalunit an astron{\ldots} |
        | abbess an abbess latin abb{\ldots} |
        +-------------------------------+
        [72269 rows x 1 columns]
        Note: Only the head of the SFrame is printed.
        You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \hypertarget{transformations}{%
\subsubsection{Transformations}\label{transformations}}

    https://dato.com/learn/userguide/text/analysis.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{dir}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} ['\_SArray\_\_check\_min\_observations',
         '\_\_abs\_\_',
         '\_\_add\_\_',
         '\_\_and\_\_',
         '\_\_bool\_\_',
         '\_\_class\_\_',
         '\_\_contains\_\_',
         '\_\_copy\_\_',
         '\_\_deepcopy\_\_',
         '\_\_delattr\_\_',
         '\_\_dir\_\_',
         '\_\_div\_\_',
         '\_\_doc\_\_',
         '\_\_eq\_\_',
         '\_\_floordiv\_\_',
         '\_\_format\_\_',
         '\_\_ge\_\_',
         '\_\_get\_content\_identifier\_\_',
         '\_\_getattribute\_\_',
         '\_\_getitem\_\_',
         '\_\_gt\_\_',
         '\_\_has\_size\_\_',
         '\_\_hash\_\_',
         '\_\_init\_\_',
         '\_\_is\_materialized\_\_',
         '\_\_iter\_\_',
         '\_\_le\_\_',
         '\_\_len\_\_',
         '\_\_lt\_\_',
         '\_\_materialize\_\_',
         '\_\_mod\_\_',
         '\_\_module\_\_',
         '\_\_mul\_\_',
         '\_\_ne\_\_',
         '\_\_neg\_\_',
         '\_\_new\_\_',
         '\_\_nonzero\_\_',
         '\_\_or\_\_',
         '\_\_pos\_\_',
         '\_\_pow\_\_',
         '\_\_proxy\_\_',
         '\_\_radd\_\_',
         '\_\_rdiv\_\_',
         '\_\_reduce\_\_',
         '\_\_reduce\_ex\_\_',
         '\_\_repr\_\_',
         '\_\_rfloordiv\_\_',
         '\_\_rmod\_\_',
         '\_\_rmul\_\_',
         '\_\_rpow\_\_',
         '\_\_rsub\_\_',
         '\_\_rtruediv\_\_',
         '\_\_setattr\_\_',
         '\_\_sizeof\_\_',
         '\_\_slots\_\_',
         '\_\_str\_\_',
         '\_\_sub\_\_',
         '\_\_subclasshook\_\_',
         '\_\_truediv\_\_',
         '\_count\_ngrams',
         '\_count\_words',
         '\_getitem\_cache',
         '\_save\_as\_text',
         'all',
         'any',
         'append',
         'apply',
         'argmax',
         'argmin',
         'astype',
         'clip',
         'clip\_lower',
         'clip\_upper',
         'contains',
         'countna',
         'cumulative\_max',
         'cumulative\_mean',
         'cumulative\_min',
         'cumulative\_std',
         'cumulative\_sum',
         'cumulative\_var',
         'date\_range',
         'datetime\_to\_str',
         'dict\_has\_all\_keys',
         'dict\_has\_any\_keys',
         'dict\_keys',
         'dict\_trim\_by\_keys',
         'dict\_trim\_by\_values',
         'dict\_values',
         'dropna',
         'dtype',
         'element\_slice',
         'explore',
         'fillna',
         'filter',
         'filter\_by',
         'from\_const',
         'from\_sequence',
         'hash',
         'head',
         'is\_in',
         'is\_materialized',
         'is\_topk',
         'item\_length',
         'materialize',
         'max',
         'mean',
         'min',
         'nnz',
         'pixel\_array\_to\_image',
         'plot',
         'random\_integers',
         'random\_split',
         'read\_json',
         'rolling\_count',
         'rolling\_max',
         'rolling\_mean',
         'rolling\_min',
         'rolling\_stdv',
         'rolling\_sum',
         'rolling\_var',
         'sample',
         'save',
         'shape',
         'show',
         'sort',
         'split\_datetime',
         'stack',
         'std',
         'str\_to\_datetime',
         'sum',
         'summary',
         'tail',
         'to\_numpy',
         'unique',
         'unpack',
         'value\_counts',
         'var',
         'vector\_slice',
         'where']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{bow} \PY{o}{=} \PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{\PYZus{}count\PYZus{}words}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n+nb}{type}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} turicreate.data\_structures.sarray.SArray
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n+nb}{type}\PY{p}{(}\PY{n}{bow}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} turicreate.data\_structures.sarray.SArray
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{bow}\PY{o}{.}\PY{n}{dict\PYZus{}has\PYZus{}any\PYZus{}keys}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{limited}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} dtype: int
         Rows: 72269
         [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, {\ldots} ]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{bow}\PY{o}{.}\PY{n}{dict\PYZus{}values}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{sf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} Columns:
         	X1	str
         
         Rows: 72269
         
         Data:
         +-------------------------------+
         |               X1              |
         +-------------------------------+
         | aynrand born and educated {\ldots} |
         | asphalt in american englis{\ldots} |
         | actinopterygii the actinop{\ldots} |
         | altaiclanguages these lang{\ldots} |
         | argon the name argon is de{\ldots} |
         | augustderleth a 1938 gugge{\ldots} |
         | amateur amateurism can be {\ldots} |
         | assemblyline an assembly l{\ldots} |
         | astronomicalunit an astron{\ldots} |
         | abbess an abbess latin abb{\ldots} |
         +-------------------------------+
         [72269 rows x 1 columns]
         Note: Only the head of the SFrame is printed.
         You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{bow}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{sf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} Columns:
         	X1	str
         	bow	dict
         
         Rows: 72269
         
         Data:
         +-------------------------------+-------------------------------+
         |               X1              |              bow              |
         +-------------------------------+-------------------------------+
         | aynrand born and educated {\ldots} | \{'spoke': 1, '5000': 1, 'f{\ldots} |
         | asphalt in american englis{\ldots} | \{'lain': 1, 'commonly': 4,{\ldots} |
         | actinopterygii the actinop{\ldots} | \{'what': 1, 'follows': 1, {\ldots} |
         | altaiclanguages these lang{\ldots} | \{'follows': 2, 'has': 11, {\ldots} |
         | argon the name argon is de{\ldots} | \{'commonly': 1, 'lattice':{\ldots} |
         | augustderleth a 1938 gugge{\ldots} | \{'rescue': 2, 'amoral': 1,{\ldots} |
         | amateur amateurism can be {\ldots} | \{'receiving': 1, 'having':{\ldots} |
         | assemblyline an assembly l{\ldots} | \{'consider': 1, 'world': 1{\ldots} |
         | astronomicalunit an astron{\ldots} | \{'given': 1, 'ephemeris': {\ldots} |
         | abbess an abbess latin abb{\ldots} | \{'major': 1, 'abbess': 10,{\ldots} |
         +-------------------------------+-------------------------------+
         [72269 rows x 2 columns]
         Note: Only the head of the SFrame is printed.
         You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n+nb}{type}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} turicreate.data\_structures.sarray.SArray
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} 72269
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} [('spoke', 1), ('5000', 1), ('follows', 1)]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tfidf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{text\PYZus{}analytics}\PY{o}{.}\PY{n}{tf\PYZus{}idf}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{sf}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} Columns:
         	X1	str
         	bow	dict
         	tfidf	dict
         
         Rows: 72269
         
         Data:
         +-------------------------------+-------------------------------+
         |               X1              |              bow              |
         +-------------------------------+-------------------------------+
         | aynrand born and educated {\ldots} | \{'spoke': 1, '5000': 1, 'f{\ldots} |
         | asphalt in american englis{\ldots} | \{'lain': 1, 'commonly': 4,{\ldots} |
         | actinopterygii the actinop{\ldots} | \{'what': 1, 'follows': 1, {\ldots} |
         | altaiclanguages these lang{\ldots} | \{'follows': 2, 'has': 11, {\ldots} |
         | argon the name argon is de{\ldots} | \{'commonly': 1, 'lattice':{\ldots} |
         | augustderleth a 1938 gugge{\ldots} | \{'rescue': 2, 'amoral': 1,{\ldots} |
         | amateur amateurism can be {\ldots} | \{'receiving': 1, 'having':{\ldots} |
         | assemblyline an assembly l{\ldots} | \{'consider': 1, 'world': 1{\ldots} |
         | astronomicalunit an astron{\ldots} | \{'given': 1, 'ephemeris': {\ldots} |
         | abbess an abbess latin abb{\ldots} | \{'major': 1, 'abbess': 10,{\ldots} |
         +-------------------------------+-------------------------------+
         +-------------------------------+
         |             tfidf             |
         +-------------------------------+
         | \{'spoke': 4.83030828067305{\ldots} |
         | \{'lain': 8.480100346078947{\ldots} |
         | \{'what': 2.505781957805935{\ldots} |
         | \{'follows': 7.511333478528{\ldots} |
         | \{'commonly': 3.77177206798{\ldots} |
         | \{'rescue': 9.1902120260774{\ldots} |
         | \{'receiving': 4.1626122325{\ldots} |
         | \{'consider': 4.33696561968{\ldots} |
         | \{'given': 2.56822027831380{\ldots} |
         | \{'major': 2.35687680945860{\ldots} |
         +-------------------------------+
         [72269 rows x 3 columns]
         Note: Only the head of the SFrame is printed.
         You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tfidf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} [('spoke', 4.830308280673057),
          ('5000', 4.791220891965009),
          ('follows', 3.7556667392640373),
          ('given', 2.5682202783138064),
          ('percent', 17.481279902908025)]
\end{Verbatim}
            
    \hypertarget{text-cleaning}{%
\subsubsection{Text cleaning}\label{text-cleaning}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{docs} \PY{o}{=} \PY{n}{sf}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dict\PYZus{}trim\PYZus{}by\PYZus{}values}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{docs} \PY{o}{=} \PY{n}{docs}\PY{o}{.}\PY{n}{dict\PYZus{}trim\PYZus{}by\PYZus{}keys}\PY{p}{(}
             \PY{n}{tc}\PY{o}{.}\PY{n}{text\PYZus{}analytics}\PY{o}{.}\PY{n}{stop\PYZus{}words}\PY{p}{(}\PY{p}{)}\PY{p}{,}
             \PY{n}{exclude}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{topic-modeling}{%
\section{Topic modeling}\label{topic-modeling}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{help}\PY{p}{(}\PY{n}{tc}\PY{o}{.}\PY{n}{topic\PYZus{}model}\PY{o}{.}\PY{n}{create}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Help on function create in module turicreate.toolkits.topic\_model.topic\_model:

create(dataset, num\_topics=10, initial\_topics=None, alpha=None, beta=0.1, num\_iterations=10, num\_burnin=5, associations=None, verbose=False, print\_interval=10, validation\_set=None, method='auto')
    Create a topic model from the given data set. A topic model assumes each
    document is a mixture of a set of topics, where for each topic some words
    are more likely than others. One statistical approach to do this is called a
    "topic model". This method learns a topic model for the given document
    collection.
    
    Parameters
    ----------
    dataset : SArray of type dict or SFrame with a single column of type dict
        A bag of words representation of a document corpus.
        Each element is a dictionary representing a single document, where
        the keys are words and the values are the number of times that word
        occurs in that document.
    
    num\_topics : int, optional
        The number of topics to learn.
    
    initial\_topics : SFrame, optional
        An SFrame with a column of unique words representing the vocabulary
        and a column of dense vectors representing
        probability of that word given each topic. When provided,
        these values are used to initialize the algorithm.
    
    alpha : float, optional
        Hyperparameter that controls the diversity of topics in a document.
        Smaller values encourage fewer topics per document.
        Provided value must be positive. Default value is 50/num\_topics.
    
    beta : float, optional
        Hyperparameter that controls the diversity of words in a topic.
        Smaller values encourage fewer words per topic. Provided value
        must be positive.
    
    num\_iterations : int, optional
        The number of iterations to perform.
    
    num\_burnin : int, optional
        The number of iterations to perform when inferring the topics for
        documents at prediction time.
    
    verbose : bool, optional
        When True, print most probable words for each topic while printing
        progress.
    
    print\_interval : int, optional
        The number of iterations to wait between progress reports.
    
    associations : SFrame, optional
        An SFrame with two columns named "word" and "topic" containing words
        and the topic id that the word should be associated with. These words
        are not considered during learning.
    
    validation\_set : SArray of type dict or SFrame with a single column
        A bag of words representation of a document corpus, similar to the
        format required for `dataset`. This will be used to monitor model
        performance during training. Each document in the provided validation
        set is randomly split: the first portion is used estimate which topic
        each document belongs to, and the second portion is used to estimate
        the model's performance at predicting the unseen words in the test data.
    
    method : \{'cgs', 'alias'\}, optional
        The algorithm used for learning the model.
    
        - *cgs:* Collapsed Gibbs sampling
        - *alias:* AliasLDA method.
    
    Returns
    -------
    out : TopicModel
        A fitted topic model. This can be used with
        :py:func:`\textasciitilde{}TopicModel.get\_topics()` and
        :py:func:`\textasciitilde{}TopicModel.predict()`. While fitting is in progress, several
        metrics are shown, including:
    
        +------------------+---------------------------------------------------+
        |      Field       | Description                                       |
        +==================+===================================================+
        | Elapsed Time     | The number of elapsed seconds.                    |
        +------------------+---------------------------------------------------+
        | Tokens/second    | The number of unique words processed per second   |
        +------------------+---------------------------------------------------+
        | Est. Perplexity  | An estimate of the model's ability to model the   |
        |                  | training data. See the documentation on evaluate. |
        +------------------+---------------------------------------------------+
    
    See Also
    --------
    TopicModel, TopicModel.get\_topics, TopicModel.predict,
    turicreate.SArray.dict\_trim\_by\_keys, TopicModel.evaluate
    
    References
    ----------
    - `Wikipedia - Latent Dirichlet allocation
      <http://en.wikipedia.org/wiki/Latent\_Dirichlet\_allocation>`\_
    
    - Alias method: Li, A. et al. (2014) `Reducing the Sampling Complexity of
      Topic Models. <http://www.sravi.org/pubs/fastlda-kdd2014.pdf>`\_.
      KDD 2014.
    
    Examples
    --------
    The following example includes an SArray of documents, where
    each element represents a document in "bag of words" representation
    -- a dictionary with word keys and whose values are the number of times
    that word occurred in the document:
    
    >>> docs = turicreate.SArray('https://static.turi.com/datasets/nytimes')
    
    Once in this form, it is straightforward to learn a topic model.
    
    >>> m = turicreate.topic\_model.create(docs)
    
    It is also easy to create a new topic model from an old one  -- whether
    it was created using Turi Create or another package.
    
    >>> m2 = turicreate.topic\_model.create(docs, initial\_topics=m['topics'])
    
    To manually fix several words to always be assigned to a topic, use
    the `associations` argument. The following will ensure that topic 0
    has the most probability for each of the provided words:
    
    >>> from turicreate import SFrame
    >>> associations = SFrame(\{'word':['hurricane', 'wind', 'storm'],
                               'topic': [0, 0, 0]\})
    >>> m = turicreate.topic\_model.create(docs,
                                        associations=associations)
    
    More advanced usage allows you  to control aspects of the model and the
    learning method.
    
    >>> import turicreate as tc
    >>> m = tc.topic\_model.create(docs,
                                  num\_topics=20,       \# number of topics
                                  num\_iterations=10,   \# algorithm parameters
                                  alpha=.01, beta=.1)  \# hyperparameters
    
    To evaluate the model's ability to generalize, we can create a train/test
    split where a portion of the words in each document are held out from
    training.
    
    >>> train, test = tc.text\_analytics.random\_split(.8)
    >>> m = tc.topic\_model.create(train)
    >>> results = m.evaluate(test)
    >>> print results['perplexity']


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{help}\PY{p}{(}\PY{n}{tc}\PY{o}{.}\PY{n}{text\PYZus{}analytics}\PY{o}{.}\PY{n}{random\PYZus{}split}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Help on function random\_split in module turicreate.toolkits.text\_analytics.\_util:

random\_split(dataset, prob=0.5)
    Utility for performing a random split for text data that is already in
    bag-of-words format. For each (word, count) pair in a particular element,
    the counts are uniformly partitioned in either a training set or a test
    set.
    
    Parameters
    ----------
    dataset : SArray of type dict, SFrame with columns of type dict
        A data set in bag-of-words format.
    
    prob : float, optional
        Probability for sampling a word to be placed in the test set.
    
    Returns
    -------
    train, test : SArray
        Two data sets in bag-of-words format, where the combined counts are
        equal to the counts in the original data set.
    
    Examples
    --------
    >>> docs = turicreate.SArray([\{'are':5, 'you':3, 'not': 1, 'entertained':10\}])
    >>> train, test = turicreate.text\_analytics.random\_split(docs)
    >>> print(train)
    [\{'not': 1.0, 'you': 3.0, 'are': 3.0, 'entertained': 7.0\}]
    >>> print(test)
    [\{'are': 2.0, 'entertained': 3.0\}]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{train}\PY{p}{,} \PY{n}{test} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{text\PYZus{}analytics}\PY{o}{.}\PY{n}{random\PYZus{}split}\PY{p}{(}\PY{n}{docs}\PY{p}{,} \PY{o}{.}\PY{l+m+mi}{8}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{m} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{topic\PYZus{}model}\PY{o}{.}\PY{n}{create}\PY{p}{(}\PY{n}{train}\PY{p}{,} 
                                         \PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}       \PY{c+c1}{\PYZsh{} number of topics}
                                         \PY{n}{num\PYZus{}iterations}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}   \PY{c+c1}{\PYZsh{} algorithm parameters}
                                         \PY{n}{alpha}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{beta}\PY{o}{=}\PY{o}{.}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} hyperparameters}
\end{Verbatim}


    
    \begin{verbatim}
Learning a topic model
    \end{verbatim}

    
    
    \begin{verbatim}
       Number of documents     72269
    \end{verbatim}

    
    
    \begin{verbatim}
           Vocabulary size    108205
    \end{verbatim}

    
    
    \begin{verbatim}
   Running collapsed Gibbs sampling
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| Iteration | Elapsed Time  | Tokens/Second  | Est. Perplexity |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| 10        | 1.66s         | 6.17013e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 20        | 3.12s         | 6.57117e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 30        | 4.58s         | 6.41968e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 40        | 6.00s         | 6.61674e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 50        | 7.42s         | 6.53873e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 60        | 8.85s         | 6.46591e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 70        | 10.38s        | 5.92867e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 80        | 11.90s        | 6.36375e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 90        | 13.42s        | 6.20572e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 100       | 15.03s        | 5.4879e+06     | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{results} \PY{o}{=} \PY{n}{m}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{test}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{results}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{perplexity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
4552.105441414741

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{m}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}33}]:} Class                          : TopicModel
         
         Schema
         ------
         Vocabulary Size                : 108205
         
         Settings
         --------
         Number of Topics               : 100
         alpha                          : 0.5
         beta                           : 0.1
         Iterations                     : 100
         Training time                  : 16.0432
         Verbose                        : True
         
         Accessible fields             : 
         m.topics                      : An SFrame containing the topics.
         m.vocabulary                  : An SArray containing the words in the vocabulary.
         Useful methods                : 
         m.get\_topics()                : Get the most probable words per topic.
         m.predict(new\_docs)           : Make predictions for new documents.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{m}\PY{o}{.}\PY{n}{get\PYZus{}topics}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} Columns:
         	topic	int
         	word	str
         	score	float
         
         Rows: 500
         
         Data:
         +-------+----------+-----------------------+
         | topic |   word   |         score         |
         +-------+----------+-----------------------+
         |   0   |  years   |  0.004647514462204498 |
         |   0   |  evans   |  0.004059221492305195 |
         |   0   | lebanon  | 0.0028172696669622205 |
         |   0   |  green   | 0.0028172696669622205 |
         |   0   |   time   | 0.0020982449259741827 |
         |   1   | national |  0.005351237598960527 |
         |   1   |   back   |  0.002278117379832135 |
         |   1   | baldwin  | 0.0018772756121197358 |
         |   1   | chicago  | 0.0018104686508343358 |
         |   1   | private  | 0.0016100477669781365 |
         +-------+----------+-----------------------+
         [500 rows x 3 columns]
         Note: Only the head of the SFrame is printed.
         You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{help}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{get\PYZus{}topics}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Help on method get\_topics in module turicreate.toolkits.topic\_model.topic\_model:

get\_topics(topic\_ids=None, num\_words=5, cdf\_cutoff=1.0, output\_type='topic\_probabilities') method of turicreate.toolkits.topic\_model.topic\_model.TopicModel instance
    Get the words associated with a given topic. The score column is the
    probability of choosing that word given that you have chosen a
    particular topic.
    
    Parameters
    ----------
    topic\_ids : list of int, optional
        The topics to retrieve words. Topic ids are zero-based.
        Throws an error if greater than or equal to m['num\_topics'], or
        if the requested topic name is not present.
    
    num\_words : int, optional
        The number of words to show.
    
    cdf\_cutoff : float, optional
        Allows one to only show the most probable words whose cumulative
        probability is below this cutoff. For example if there exist
        three words where
    
        .. math::
           p(word\_1 | topic\_k) = .1
    
           p(word\_2 | topic\_k) = .2
    
           p(word\_3 | topic\_k) = .05
    
        then setting :math:`cdf\_\{cutoff\}=.3` would return only
        :math:`word\_1` and :math:`word\_2` since
        :math:`p(word\_1 | topic\_k) + p(word\_2 | topic\_k) <= cdf\_\{cutoff\}`
    
    output\_type : \{'topic\_probabilities' | 'topic\_words'\}, optional
        Determine the type of desired output. See below.
    
    Returns
    -------
    out : SFrame
        If output\_type is 'topic\_probabilities', then the returned value is
        an SFrame with a column of words ranked by a column of scores for
        each topic. Otherwise, the returned value is a SArray where
        each element is a list of the most probable words for each topic.
    
    Examples
    --------
    Get the highest ranked words for all topics.
    
    >>> docs = turicreate.SArray('https://static.turi.com/datasets/nips-text')
    >>> m = turicreate.topic\_model.create(docs,
                                        num\_iterations=50)
    >>> m.get\_topics()
    +-------+----------+-----------------+
    | topic |   word   |      score      |
    +-------+----------+-----------------+
    |   0   |   cell   |  0.028974400831 |
    |   0   |  input   | 0.0259470208503 |
    |   0   |  image   | 0.0215721599763 |
    |   0   |  visual  | 0.0173635081992 |
    |   0   |  object  | 0.0172447874156 |
    |   1   | function | 0.0482834508265 |
    |   1   |  input   | 0.0456270024091 |
    |   1   |  point   | 0.0302662839454 |
    |   1   |  result  | 0.0239474934631 |
    |   1   | problem  | 0.0231750116011 |
    |  {\ldots}  |   {\ldots}    |       {\ldots}       |
    +-------+----------+-----------------+
    
    Get the highest ranked words for topics 0 and 1 and show 15 words per
    topic.
    
    >>> m.get\_topics([0, 1], num\_words=15)
    +-------+----------+------------------+
    | topic |   word   |      score       |
    +-------+----------+------------------+
    |   0   |   cell   |  0.028974400831  |
    |   0   |  input   | 0.0259470208503  |
    |   0   |  image   | 0.0215721599763  |
    |   0   |  visual  | 0.0173635081992  |
    |   0   |  object  | 0.0172447874156  |
    |   0   | response | 0.0139740298286  |
    |   0   |  layer   | 0.0122585145062  |
    |   0   | features | 0.0115343177265  |
    |   0   | feature  | 0.0103530459301  |
    |   0   | spatial  | 0.00823387994361 |
    |  {\ldots}  |   {\ldots}    |       {\ldots}        |
    +-------+----------+------------------+
    
    If one wants to instead just get the top words per topic, one may
    change the format of the output as follows.
    
    >>> topics = m.get\_topics(output\_type='topic\_words')
    dtype: list
    Rows: 10
    [['cell', 'image', 'input', 'object', 'visual'],
     ['algorithm', 'data', 'learning', 'method', 'set'],
     ['function', 'input', 'point', 'problem', 'result'],
     ['model', 'output', 'pattern', 'set', 'unit'],
     ['action', 'learning', 'net', 'problem', 'system'],
     ['error', 'function', 'network', 'parameter', 'weight'],
     ['information', 'level', 'neural', 'threshold', 'weight'],
     ['control', 'field', 'model', 'network', 'neuron'],
     ['hidden', 'layer', 'system', 'training', 'vector'],
     ['component', 'distribution', 'local', 'model', 'optimal']]


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{topics} \PY{o}{=} \PY{n}{m}\PY{o}{.}\PY{n}{get\PYZus{}topics}\PY{p}{(}\PY{n}{num\PYZus{}words}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{unstack}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PYZbs{}
                                         \PY{n}{new\PYZus{}column\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{topic\PYZus{}words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{topic\PYZus{}words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{k}{for} \PY{n}{topic} \PY{o+ow}{in} \PY{n}{topics}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{topic}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['storm', 'florida', 'area', 'due', 'texas', 'people', 'damage', 'hurricane', 'tropical', 'system']
['project', 'ryan', 'founded', 'harvard', 'carroll', 'including', 'wilson', 'school', 'oxford', 'national']
['alliance', 'organization', 'membership', 'board', 'member', 'members', 'association', 'groups', 'group', 'society']
['summer', 'george', 'kan', 'julian', '1978', 'date', 'years', 'wolfe', 'london', 'italian']
['german', 'division', 'forces', 'british', 'army', 'men', 'battle', 'general', 'war', 'military']
['arrow', 'williams', 'california', 'warren', 'santa', 'los', 'san', 'great', 'angeles', 'renamed']
['miller', 'time', 'produced', 'years', 'lewis', 'hamilton', 'morris', '1999', '2005', 'epic']
['played', 'games', 'coach', 'won', 'points', 'team', 'game', 'record', 'season', 'teams']
['green', 'time', 'connecticut', 'evans', 'years', 'sixth', 'lebanon', '2001', 'worked', '2005']
['van', 'berlin', 'im', 'des', 'von', 'die', 'den', 'der', 'das', 'und']
['clothing', 'hat', 'worn', 'wear', 'made', 'green', 'summer', 'black', 'top', 'dress']
['2009', 'year', 'kids', 'october', '2010', '2007', 'school', '2006', '2008', 'national']
['magazine', 'grant', 'oregon', 'portland', 'college', '1985', '2006', 'year', 'long', 'beer']
['network', 'show', 'local', 'news', 'broadcast', 'radio', 'channel', 'stations', 'television', 'station']
['bridge', 'north', 'river', 'county', 'state', 'west', 'route', 'city', 'east', 'road']
['part', 'degree', 'na', 'valid', 'agent', 'history', 'reform', 'born', 'agency', 'trn']
['children', 'father', 'mother', 'married', 'years', 'family', 'born', 'life', 'died', 'time']
['early', 'population', 'century', 'american', 'war', 'government', 'states', 'british', 'united', 'people']
['stories', 'magazine', 'writing', 'history', 'work', 'wrote', 'works', 'published', 'book', 'books']
['collection', 'painting', 'arts', 'museum', 'work', 'york', 'design', 'artists', 'art', 'works']
['named', 'flag', 'orange', 'blue', 'colors', 'yellow', 'tiger', 'black', 'red', 'white']
['made', 'mission', 'test', 'project', 'space', 'aircraft', 'wright', 'design', 'launch', 'flight']
['alan', 'phillips', 'preov', 'koice', 'beth', 'trenn', 'phillip', 'nitra', 'town', 'trnava']
['racing', 'cars', 'car', 'engines', 'models', 'series', 'engine', 'system', 'race', 'model']
['building', 'duck', 'london', 'philadelphia', 'represented', 'major', 'york', 'design', 'journal', 'duncan']
['english', 'language', 'form', 'meaning', 'word', 'words', 'names', 'written', 'languages', 'common']
['gene', 'enzyme', 'dna', 'site', 'proteins', 'protein', 'cells', 'cell', 'structure', 'genes']
['work', 'year', '2009', 'andrew', 'list', 'henin', 'link', 'dates', 'part', 'calendar']
['law', 'public', 'united', 'state', 'legal', 'act', 'states', 'court', 'case', 'federal']
['trains', 'west', 'line', 'rail', 'train', 'station', 'services', 'service', 'opened', 'railway']
['served', 'cemetery', 'received', 'texas', 'po', 'christi', 'university', 'post', 'corpus', 'mexico']
['international', 'worked', 'festival', 'director', 'film', 'awards', 'won', 'award', 'awarded', 'academy']
['empire', 'china', 'chinese', 'dynasty', 'roman', 'greek', 'time', 'king', 'bc', 'emperor']
['port', 'ship', 'coast', 'island', 'sea', 'fleet', 'region', 'bay', 'ships', 'islands']
['south', 'north', 'river', 'mountain', 'water', 'creek', 'lake', 'valley', 'park', 'area']
['january', 'day', 'june', '2009', 'october', 'december', '2010', '2007', '2008', 'september']
['round', 'team', 'won', 'event', 'title', 'world', 'match', 'championship', 'lost', 'open']
['created', '2010', 'ash', 'complex', 'sanders', 'hotel', 'skating', 'school', 'house', 'msn']
['left', 'easy', 'leg', 'ancient', 'benson', 'school', '2003', 'critical', 'british', 'westfield']
['released', 'world', 'games', 'version', 'series', 'video', 'player', 'game', 'players', '2']
['won', 'cup', 'team', 'teams', 'season', 'final', 'football', 'club', 'league', 'played']
['1988', 'called', 'list', '1971', '2010', '1994', 'made', 'trent', 'found', 'local']
['world', 'received', 'usa', 'wall', 'part', 'start0', 'williams', 'work', 'events', 'plotdata']
['years', 'age', 'income', 'median', '18', 'living', 'males', 'town', 'average', 'population']
['youth', 'bamboo', 'ross', 'griffin', 'williams', 'meeting', 'monument', 'town', 'joined', 'girl']
['south', 'european', 'international', 'economic', 'states', 'development', 'united', 'government', 'countries', 'world']
['national', 'constituencies', 'animation', 'part', 'track', 'baldwin', '1982', 'private', 'back', 'chicago']
['hill', 'morgan', 'davis', 'cj', '1992', 'arch', 'child', '1995', 'tony', 'part']
['form', 'called', '1', 'theory', 'set', 'number', 'function', 'type', 'system', 'model']
['form', 'women', 'social', 'life', 'society', 'people', 'time', 'world', 'human', 'work']
['city', 'san', 'paris', 'el', 'france', 'la', 'de', 'french', 'spain', 'spanish']
['city', 'located', 'house', 'park', 'building', 'street', 'built', 'town', 'area', 'century']
['made', 'include', 'variety', 'traditional', 'rice', 'popular', 'food', 'called', 'served', 'wine']
['canadian', 'ontario', 'alberta', 'british', 'york', '2002', 'toronto', 'quebec', 'canada', 'montreal']
['khan', 'afghanistan', 'indian', 'singh', 'temple', 'punjab', 'pakistan', 'sri', 'india', 'community']
['brooklyn', 'city', 'class', 'ambassador', '2004', 'due', 'babylon', 'historical', 'area', '5']
['heat', 'iron', 'high', 'process', 'water', 'form', 'gas', 'temperature', 'nuclear', 'energy']
['airlines', 'regional', 'airport', '2010', 'international', 'travel', 'airways', '2005', 'airline', 'joined']
['head', '15', 'home', '2010', '2003', 'elected', 'park', 'stone', 'gaol', 'numerous']
['poland', 'years', 'polish', 'soviet', 'moscow', 'war', 'russian', 'swedish', 'union', 'russia']
['william', 'henry', 'england', 'sir', 'london', 'royal', 'lord', 'king', 'son', 'john']
['ohio', 'virginia', 'district', 'washington', 'john', 'carolina', 'county', 'state', 'served', 'south']
['state', 'election', 'political', 'minister', 'council', 'national', 'party', 'president', 'government', 'elected']
['district', 'districts', 'register', 'properties', 'places', 'illinois', 'national', 'county', 'historic', 'school']
['operations', 'aircraft', 'base', 'united', 'war', 'service', 'air', 'group', 'force', 'training']
['number', '25', '2010', 'golf', 'time', 'scrooge', 'group', 'university', 'chams', 'world']
['light', 'range', 'made', 'time', 'system', 'small', 'power', 'energy', 'high', 'current']
['1983', 'university', 'states', 'served', 'appointed', '1986', 'degree', 'bill', 'professor', '2000']
['years', 'head', 'joseph', 'school', 'smiths', 'rejoice', 'smith', 'clark', 'stone', 'plates']
['found', 'occur', 'body', 'small', 'disease', 'large', 'species', 'family', 'order', 'birds']
['number', 'fat', '2006', 'named', 'work', 'death', 'published', 'flores', 'vincent', 'villa']
['company', 'business', 'mine', 'sold', 'oil', 'stores', 'industry', 'year', 'production', 'mining']
['martin', 'kitty', 'obesity', 'turtle', 'point', '2000', 'camp', 'clark', 'moved', 'born']
['1994', '1998', 'ryo', 'venezuela', 'tannins', 'national', 'group', 'minor', 'christmas', 'world']
['parish', 'saint', 'bishop', 'church', 'roman', 'council', 'st', 'century', 'catholic', 'pope']
['released', 'records', 'single', 'album', 'songs', 'music', 'band', 'live', 'song', 'tour']
['wales', 'australia', 'played', 'cricket', 'zealand', 'day', 'sydney', 'made', 'australian', 'south']
['information', 'network', 'system', 'software', 'systems', 'internet', 'users', 'technology', 'computer', 'data']
['philippine', 'clara', 'foundation', 'chief', 'full', 'fair', 'philippines', 'manila', 'rupiah', 'belle']
['group', 'began', 'anderson', 'minnesota', 'john', 'period', 'jones', 'te', 'td', 'dont']
['northern', 'cork', 'dublin', 'ireland', 'irish', 'senior', 'john', 'county', 'title', 'medal']
['school', '2006', 'stone', 'york', 'bay', 'village', 'held', 'created', 'local', 'community']
['years', 'jersey', 'police', 'city', 'york', 'family', 'crime', 'gang', 'prison', 'arrested']
['2001', 'american', 'saudi', 'bin', 'world', 'al', 'sh', 'including', 'laden', 'joined']
['school', 'education', 'year', 'students', 'research', 'schools', 'college', 'high', 'university', 'program']
['events', 'time', 'mr', 'ride', 'roller', 'part', 'including', 'bicycle', 'riders', 'bike']
['montenegro', 'yugoslavia', 'serbia', 'school', 'serbian', 'albanian', 'albania', 'croatian', 'croatia', 'year']
['austin', 'shells', 'place', 'peter', 'shell', 'uk', 'school', '2010', 'active', 'mark']
['york', 'series', 'masters', 'booth', 'world', 'free', '2003', 'major', 'birmingham', 'setting']
['role', 'appeared', 'films', 'episode', 'television', 'show', 'series', 'character', 'movie', 'film']
['medicine', 'dr', 'training', 'care', 'hospital', 'health', 'surgery', 'center', 'dog', 'medical']
['norwegian', 'travis', '2007', 'ste', 'bar', 'city', 'home', 'norway', 'parker', 'include']
['jewish', 'christian', 'people', 'god', 'gods', 'religious', 'great', 'church', 'churches', 'jesus']
['zion', 'contest', 'miss', 'relay', 'gold', 'post', 'nec', 'mori', 'yacht', 'time']
['york', 'theatre', 'theater', 'dance', 'musical', 'orchestra', 'opera', 'music', 'performed', 'festival']
['tax', 'money', 'financial', 'market', 'bank', 'million', 'business', 'price', 'services', 'company']
['life', 'death', 'father', 'end', 'find', 'back', 'man', 'make', 'time', 'tells']
['years', '2007', 'walker', 'post', '1911', 'croydon', 'burma', 'burmese', 'named', 'day']
['high', 'vegas', 'purchased', 'alice', 'paul', 'hotel', 'scott', 'ranch', 'las', 'highlands']
['horse', 'stakes', 'born', 'american', 'horses', 'race', 'boas', 'breed', 'english', 'racing']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{help}\PY{p}{(}\PY{n}{m}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Help on TopicModel in module turicreate.toolkits.topic\_model.topic\_model object:

class TopicModel(turicreate.toolkits.\_model.Model)
 |  TopicModel objects can be used to predict the underlying topic of a
 |  document.
 |  
 |  This model cannot be constructed directly.  Instead, use
 |  :func:`turicreate.topic\_model.create` to create an instance
 |  of this model. A detailed list of parameter options and code samples
 |  are available in the documentation for the create function.
 |  
 |  Method resolution order:
 |      TopicModel
 |      turicreate.toolkits.\_model.Model
 |      turicreate.toolkits.\_model.ExposeAttributesFromProxy
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  \_\_init\_\_(self, model\_proxy)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  \_\_repr\_\_(self)
 |      Print a string description of the model when the model name is entered
 |      in the terminal.
 |  
 |  \_\_str\_\_(self)
 |      Return a string description of the model to the ``print`` method.
 |      
 |      Returns
 |      -------
 |      out : string
 |          A description of the model.
 |  
 |  evaluate(self, train\_data, test\_data=None, metric='perplexity')
 |      Estimate the model's ability to predict new data. Imagine you have a
 |      corpus of books. One common approach to evaluating topic models is to
 |      train on the first half of all of the books and see how well the model
 |      predicts the second half of each book.
 |      
 |      This method returns a metric called perplexity, which  is related to the
 |      likelihood of observing these words under the given model. See
 |      :py:func:`\textasciitilde{}turicreate.topic\_model.perplexity` for more details.
 |      
 |      The provided `train\_data` and `test\_data` must have the same length,
 |      i.e., both data sets must have the same number of documents; the model
 |      will use train\_data to estimate which topic the document belongs to, and
 |      this is used to estimate the model's performance at predicting the
 |      unseen words in the test data.
 |      
 |      See :py:func:`\textasciitilde{}turicreate.topic\_model.TopicModel.predict` for details
 |      on how these predictions are made, and see
 |      :py:func:`\textasciitilde{}turicreate.text\_analytics.random\_split` for a helper function
 |      that can be used for making train/test splits.
 |      
 |      Parameters
 |      ----------
 |      train\_data : SArray or SFrame
 |          A set of documents to predict topics for.
 |      
 |      test\_data : SArray or SFrame, optional
 |          A set of documents to evaluate performance on.
 |          By default this will set to be the same as train\_data.
 |      
 |      metric : str
 |          The chosen metric to use for evaluating the topic model.
 |          Currently only 'perplexity' is supported.
 |      
 |      Returns
 |      -------
 |      out : dict
 |          The set of estimated evaluation metrics.
 |      
 |      See Also
 |      --------
 |      predict, turicreate.toolkits.text\_analytics.random\_split
 |      
 |      Examples
 |      --------
 |      >>> docs = turicreate.SArray('https://static.turi.com/datasets/nips-text')
 |      >>> train\_data, test\_data = turicreate.text\_analytics.random\_split(docs)
 |      >>> m = turicreate.topic\_model.create(train\_data)
 |      >>> m.evaluate(train\_data, test\_data)
 |      \{'perplexity': 2467.530370396021\}
 |  
 |  get\_topics(self, topic\_ids=None, num\_words=5, cdf\_cutoff=1.0, output\_type='topic\_probabilities')
 |      Get the words associated with a given topic. The score column is the
 |      probability of choosing that word given that you have chosen a
 |      particular topic.
 |      
 |      Parameters
 |      ----------
 |      topic\_ids : list of int, optional
 |          The topics to retrieve words. Topic ids are zero-based.
 |          Throws an error if greater than or equal to m['num\_topics'], or
 |          if the requested topic name is not present.
 |      
 |      num\_words : int, optional
 |          The number of words to show.
 |      
 |      cdf\_cutoff : float, optional
 |          Allows one to only show the most probable words whose cumulative
 |          probability is below this cutoff. For example if there exist
 |          three words where
 |      
 |          .. math::
 |             p(word\_1 | topic\_k) = .1
 |      
 |             p(word\_2 | topic\_k) = .2
 |      
 |             p(word\_3 | topic\_k) = .05
 |      
 |          then setting :math:`cdf\_\{cutoff\}=.3` would return only
 |          :math:`word\_1` and :math:`word\_2` since
 |          :math:`p(word\_1 | topic\_k) + p(word\_2 | topic\_k) <= cdf\_\{cutoff\}`
 |      
 |      output\_type : \{'topic\_probabilities' | 'topic\_words'\}, optional
 |          Determine the type of desired output. See below.
 |      
 |      Returns
 |      -------
 |      out : SFrame
 |          If output\_type is 'topic\_probabilities', then the returned value is
 |          an SFrame with a column of words ranked by a column of scores for
 |          each topic. Otherwise, the returned value is a SArray where
 |          each element is a list of the most probable words for each topic.
 |      
 |      Examples
 |      --------
 |      Get the highest ranked words for all topics.
 |      
 |      >>> docs = turicreate.SArray('https://static.turi.com/datasets/nips-text')
 |      >>> m = turicreate.topic\_model.create(docs,
 |                                          num\_iterations=50)
 |      >>> m.get\_topics()
 |      +-------+----------+-----------------+
 |      | topic |   word   |      score      |
 |      +-------+----------+-----------------+
 |      |   0   |   cell   |  0.028974400831 |
 |      |   0   |  input   | 0.0259470208503 |
 |      |   0   |  image   | 0.0215721599763 |
 |      |   0   |  visual  | 0.0173635081992 |
 |      |   0   |  object  | 0.0172447874156 |
 |      |   1   | function | 0.0482834508265 |
 |      |   1   |  input   | 0.0456270024091 |
 |      |   1   |  point   | 0.0302662839454 |
 |      |   1   |  result  | 0.0239474934631 |
 |      |   1   | problem  | 0.0231750116011 |
 |      |  {\ldots}  |   {\ldots}    |       {\ldots}       |
 |      +-------+----------+-----------------+
 |      
 |      Get the highest ranked words for topics 0 and 1 and show 15 words per
 |      topic.
 |      
 |      >>> m.get\_topics([0, 1], num\_words=15)
 |      +-------+----------+------------------+
 |      | topic |   word   |      score       |
 |      +-------+----------+------------------+
 |      |   0   |   cell   |  0.028974400831  |
 |      |   0   |  input   | 0.0259470208503  |
 |      |   0   |  image   | 0.0215721599763  |
 |      |   0   |  visual  | 0.0173635081992  |
 |      |   0   |  object  | 0.0172447874156  |
 |      |   0   | response | 0.0139740298286  |
 |      |   0   |  layer   | 0.0122585145062  |
 |      |   0   | features | 0.0115343177265  |
 |      |   0   | feature  | 0.0103530459301  |
 |      |   0   | spatial  | 0.00823387994361 |
 |      |  {\ldots}  |   {\ldots}    |       {\ldots}        |
 |      +-------+----------+------------------+
 |      
 |      If one wants to instead just get the top words per topic, one may
 |      change the format of the output as follows.
 |      
 |      >>> topics = m.get\_topics(output\_type='topic\_words')
 |      dtype: list
 |      Rows: 10
 |      [['cell', 'image', 'input', 'object', 'visual'],
 |       ['algorithm', 'data', 'learning', 'method', 'set'],
 |       ['function', 'input', 'point', 'problem', 'result'],
 |       ['model', 'output', 'pattern', 'set', 'unit'],
 |       ['action', 'learning', 'net', 'problem', 'system'],
 |       ['error', 'function', 'network', 'parameter', 'weight'],
 |       ['information', 'level', 'neural', 'threshold', 'weight'],
 |       ['control', 'field', 'model', 'network', 'neuron'],
 |       ['hidden', 'layer', 'system', 'training', 'vector'],
 |       ['component', 'distribution', 'local', 'model', 'optimal']]
 |  
 |  predict(self, dataset, output\_type='assignment', num\_burnin=None)
 |      Use the model to predict topics for each document. The provided
 |      `dataset` should be an SArray object where each element is a dict
 |      representing a single document in bag-of-words format, where keys
 |      are words and values are their corresponding counts. If `dataset` is
 |      an SFrame, then it must contain a single column of dict type.
 |      
 |      The current implementation will make inferences about each document
 |      given its estimates of the topics learned when creating the model.
 |      This is done via Gibbs sampling.
 |      
 |      Parameters
 |      ----------
 |      dataset : SArray, SFrame of type dict
 |          A set of documents to use for making predictions.
 |      
 |      output\_type : str, optional
 |          The type of output desired. This can either be
 |      
 |          - assignment: the returned values are integers in [0, num\_topics)
 |          - probability: each returned prediction is a vector with length
 |            num\_topics, where element k represents the probability that
 |            document belongs to topic k.
 |      
 |      num\_burnin : int, optional
 |          The number of iterations of Gibbs sampling to perform when
 |          inferring the topics for documents at prediction time.
 |          If provided this will override the burnin value set during
 |          training.
 |      
 |      Returns
 |      -------
 |      out : SArray
 |      
 |      See Also
 |      --------
 |      evaluate
 |      
 |      Examples
 |      --------
 |      Make predictions about which topic each document belongs to.
 |      
 |      >>> docs = turicreate.SArray('https://static.turi.com/datasets/nips-text')
 |      >>> m = turicreate.topic\_model.create(docs)
 |      >>> pred = m.predict(docs)
 |      
 |      If one is interested in the probability of each topic
 |      
 |      >>> pred = m.predict(docs, output\_type='probability')
 |      
 |      Notes
 |      -----
 |      For each unique word w in a document d, we sample an assignment to
 |      topic k with probability proportional to
 |      
 |      .. math::
 |          p(z\_\{dw\} = k) \textbackslash{}propto (n\_\{d,k\} + \textbackslash{}alpha) * \textbackslash{}Phi\_\{w,k\}
 |      
 |      where
 |      
 |      - :math:`W` is the size of the vocabulary,
 |      - :math:`n\_\{d,k\}` is the number of other times we have assigned a word in
 |        document to d to topic :math:`k`,
 |      - :math:`\textbackslash{}Phi\_\{w,k\}` is the probability under the model of choosing word
 |        :math:`w` given the word is of topic :math:`k`. This is the matrix
 |        returned by calling `m['topics']`.
 |      
 |      This represents a collapsed Gibbs sampler for the document assignments
 |      while we keep the topics learned during training fixed.
 |      This process is done in parallel across all documents, five times per
 |      document.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from turicreate.toolkits.\_model.Model:
 |  
 |  save(self, location)
 |      Save the model. The model is saved as a directory which can then be
 |      loaded using the :py:func:`\textasciitilde{}turicreate.load\_model` method.
 |      
 |      Parameters
 |      ----------
 |      location : string
 |          Target destination for the model. Can be a local path or remote URL.
 |      
 |      See Also
 |      ----------
 |      turicreate.load\_model
 |      
 |      Examples
 |      ----------
 |      >>> model.save('my\_model\_file')
 |      >>> loaded\_model = turicreate.load\_model('my\_model\_file')
 |  
 |  summary(self, output=None)
 |      Print a summary of the model. The summary includes a description of
 |      training data, options, hyper-parameters, and statistics measured
 |      during model creation.
 |      
 |      Parameters
 |      ----------
 |      output : str, None
 |          The type of summary to return.
 |      
 |          - None or 'stdout' : print directly to stdout.
 |      
 |          - 'str' : string of summary
 |      
 |          - 'dict' : a dict with 'sections' and 'section\_titles' ordered
 |            lists. The entries in the 'sections' list are tuples of the form
 |            ('label', 'value').
 |      
 |      Examples
 |      --------
 |      >>> m.summary()
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from turicreate.toolkits.\_model.ExposeAttributesFromProxy:
 |  
 |  \_\_dir\_\_(self)
 |      Combine the results of dir from the current class with the results of
 |      list\_fields().
 |  
 |  \_\_getattribute\_\_(self, attr)
 |      Use the internal proxy object for obtaining list\_fields.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from turicreate.toolkits.\_model.ExposeAttributesFromProxy:
 |  
 |  \_\_dict\_\_
 |      dictionary for instance variables (if defined)
 |  
 |  \_\_weakref\_\_
 |      list of weak references to the object (if defined)
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from turicreate.toolkits.\_model.ExposeAttributesFromProxy:
 |  
 |  \_\_proxy\_\_ = None


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{def} \PY{n+nf}{print\PYZus{}topics}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
             \PY{n}{topics} \PY{o}{=} \PY{n}{m}\PY{o}{.}\PY{n}{get\PYZus{}topics}\PY{p}{(}\PY{n}{num\PYZus{}words}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
             \PY{n}{topics} \PY{o}{=} \PY{n}{topics}\PY{o}{.}\PY{n}{unstack}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{new\PYZus{}column\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{topic\PYZus{}words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{topic\PYZus{}words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{topics} \PY{o}{=} \PY{n}{topics}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{k}{for} \PY{n}{topic} \PY{o+ow}{in} \PY{n}{topics}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{topic}\PY{p}{)}
         \PY{n}{print\PYZus{}topics}\PY{p}{(}\PY{n}{m}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['people', 'texas', 'tropical', 'florida', 'storm']
['school', 'founded', 'wilson', 'harvard', 'including']
['members', 'association', 'society', 'member', 'group']
['summer', 'date', 'julian', 'years', 'george']
['war', 'general', 'battle', 'german', 'army']
['california', 'angeles', 'san', 'los', 'santa']
['miller', 'morris', '2005', '1999', 'time']
['game', 'season', 'team', 'points', 'games']
['evans', 'years', 'time', 'green', 'lebanon']
['und', 'der', 'die', 'van', 'von']
['wear', 'worn', 'made', 'green', 'top']
['2008', 'year', '2007', '2009', '2010']
['oregon', 'magazine', 'year', 'portland', 'beer']
['network', 'news', 'radio', 'show', 'station']
['bridge', 'north', 'route', 'west', 'road']
['part', 'born', 'na', 'agent', 'valid']
['years', 'family', 'father', 'died', 'time']
['united', 'population', 'people', 'american', 'states']
['work', 'magazine', 'published', 'book', 'books']
['collection', 'museum', 'arts', 'art', 'work']
['white', 'black', 'red', 'flag', 'blue']
['aircraft', 'design', 'space', 'project', 'flight']
['trenn', 'phillip', 'nitra', 'koice', 'preov']
['engine', 'cars', 'car', 'race', 'model']
['london', 'duck', 'journal', 'philadelphia', 'york']
['english', 'word', 'language', 'languages', 'words']
['dna', 'cell', 'cells', 'enzyme', 'protein']
['2009', 'calendar', 'year', 'link', 'list']
['court', 'act', 'state', 'states', 'law']
['station', 'services', 'railway', 'line', 'service']
['cemetery', 'university', 'mexico', 'texas', 'corpus']
['film', 'worked', 'festival', 'award', 'awards']
['chinese', 'emperor', 'bc', 'king', 'empire']
['islands', 'region', 'ships', 'island', 'ship']
['water', 'park', 'lake', 'area', 'river']
['2007', '2009', 'december', '2010', '2008']
['match', 'world', 'title', 'won', 'championship']
['msn', 'house', '2010', 'skating', 'school']
['school', 'westfield', 'easy', 'british', '2003']
['player', 'players', 'game', 'games', 'series']
['team', 'league', 'season', 'club', 'football']
['local', 'list', 'found', 'called', '2010']
['plotdata', 'received', 'world', 'part', 'usa']
['income', 'population', '18', 'years', 'age']
['youth', 'ross', 'griffin', 'joined', 'williams']
['south', 'government', 'international', 'countries', 'world']
['national', 'private', 'back', 'chicago', 'baldwin']
['cj', 'hill', 'morgan', 'davis', 'child']
['set', '1', 'model', 'theory', 'number']
['social', 'people', 'time', 'form', 'work']
['french', 'de', 'spanish', 'france', 'la']
['area', 'city', 'building', 'town', 'built']
['food', 'popular', 'made', 'called', 'wine']
['canadian', 'british', 'ontario', 'canada', 'toronto']
['india', 'temple', 'khan', 'pakistan', 'indian']
['babylon', '2004', 'area', 'class', '5']
['water', 'process', 'nuclear', 'energy', 'form']
['international', '2010', 'airport', 'airlines', 'airline']
['15', 'park', 'head', 'stone', '2003']
['soviet', 'russia', 'poland', 'russian', 'polish']
['john', 'royal', 'william', 'sir', 'king']
['virginia', 'county', 'served', 'state', 'carolina']
['election', 'government', 'president', 'state', 'party']
['school', 'county', 'national', 'districts', 'district']
['group', 'war', 'force', 'air', 'aircraft']
['world', 'university', '2010', 'chams', 'number']
['energy', 'power', 'system', 'time', 'light']
['bill', 'professor', 'degree', 'served', 'university']
['joseph', 'head', 'smiths', 'smith', 'school']
['birds', 'family', 'small', 'species', 'found']
['fat', '2006', 'named', 'work', 'vincent']
['business', 'stores', 'oil', 'company', 'mine']
['2000', 'kitty', 'camp', 'moved', 'obesity']
['1994', 'christmas', 'world', 'national', 'minor']
['bishop', 'saint', 'church', 'st', 'century']
['band', 'released', 'album', 'music', 'song']
['made', 'zealand', 'australian', 'australia', 'south']
['system', 'software', 'data', 'systems', 'information']
['full', 'fair', 'philippines', 'manila', 'philippine']
['began', 'jones', 'minnesota', 'anderson', 'td']
['county', 'ireland', 'medal', 'irish', 'dublin']
['school', 'held', 'village', 'local', 'community']
['family', 'city', 'prison', 'police', 'york']
['al', 'bin', '2001', 'joined', 'world']
['college', 'university', 'education', 'students', 'school']
['including', 'riders', 'ride', 'time', 'part']
['year', 'yugoslavia', 'serbian', 'albanian', 'serbia']
['shells', 'uk', 'school', 'austin', 'shell']
['world', 'birmingham', 'york', 'booth', 'free']
['show', 'role', 'episode', 'series', 'film']
['dr', 'care', 'health', 'medical', 'hospital']
['include', '2007', 'norway', 'norwegian', 'city']
['religious', 'christian', 'church', 'god', 'jesus']
['nec', 'mori', 'miss', 'post', 'time']
['festival', 'theatre', 'dance', 'music', 'opera']
['business', 'company', 'financial', 'bank', 'million']
['man', 'tells', 'life', 'back', 'time']
['years', '2007', 'post', 'croydon', 'day']
['las', 'scott', 'vegas', 'ranch', 'hotel']
['american', 'horses', 'horse', 'breed', 'stakes']

    \end{Verbatim}

    \begin{quote}
pred = m.predict(another\_data)
\end{quote}

\begin{quote}
pred = m.predict(another\_data, output\_type=`probabilities')
\end{quote}

    \hypertarget{initializing-from-other-models}{%
\subsubsection{Initializing from other
models}\label{initializing-from-other-models}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n+nb}{dir}\PY{p}{(}\PY{n}{m}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} ['\_\_class\_\_',
          '\_\_delattr\_\_',
          '\_\_dict\_\_',
          '\_\_dir\_\_',
          '\_\_doc\_\_',
          '\_\_eq\_\_',
          '\_\_format\_\_',
          '\_\_ge\_\_',
          '\_\_getattribute\_\_',
          '\_\_gt\_\_',
          '\_\_hash\_\_',
          '\_\_init\_\_',
          '\_\_le\_\_',
          '\_\_lt\_\_',
          '\_\_module\_\_',
          '\_\_ne\_\_',
          '\_\_new\_\_',
          '\_\_proxy\_\_',
          '\_\_reduce\_\_',
          '\_\_reduce\_ex\_\_',
          '\_\_repr\_\_',
          '\_\_setattr\_\_',
          '\_\_sizeof\_\_',
          '\_\_str\_\_',
          '\_\_subclasshook\_\_',
          '\_\_weakref\_\_',
          '\_get',
          '\_get\_queryable\_methods',
          '\_get\_summary\_struct',
          '\_list\_fields',
          '\_name',
          '\_native\_name',
          '\_training\_stats',
          'alpha',
          'beta',
          'evaluate',
          'get\_topics',
          'num\_burnin',
          'num\_iterations',
          'num\_topics',
          'predict',
          'print\_interval',
          'save',
          'summary',
          'topics',
          'training\_iterations',
          'training\_time',
          'validation\_time',
          'verbose',
          'vocabulary']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{m}\PY{o}{.}\PY{n}{vocabulary}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:} dtype: str
         Rows: 108205
         ['book', 'political', 'time', 'readers', 'individual', 'appeared', 'peikoff', 'concepts', '100', 'picture', 'america', 'reviewers', 'philosopher', 'screenwriter', 'work', 'traditional', 'purged', 'ayn', 'york', 'articles', 'pharmacy', 'scholarship', '2001', 'designed', 'permission', 'taking', 'historian', 'library', 'russian', 'extent', 'childhood', 'respondents', 'language', 'alisa', 'writing', 'union', 'libertarian', 'positive', 'jennifer', 'notes', 'line', 'burns', 'state', 'crimea', 'sciabarra', 'based', 'rights', 'life', 'shes', 'argument', 'nonfiction', 'rejection', 'allowed', 'reason', 'culture', 'closest', 'shrugged', 'free', 'january', 'success', 'living', 'robert', 'literary', 'animated', 'american', 'reviews', 'paterson', 'people', 'percent', 'house', 'academic', 'sacrificing', 'referred', 'broadway', 'fountainhead', 'lectures', 'john', 'inspiration', 'conditions', 'lifetime', 'written', '1938', 'established', 'barbara', 'twelve', 'modern', 'final', 'intellectual', 'audience', 'stated', 'selfinterest', 'achievement', 'century', 'relationship', 'allowing', 'delivering', 'writers', 'influence', 'branden', 'film', {\ldots} ]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{m}\PY{o}{.}\PY{n}{topics}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} 108205
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{m2} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{topic\PYZus{}model}\PY{o}{.}\PY{n}{create}\PY{p}{(}\PY{n}{docs}\PY{p}{,}
                                          \PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
                                          \PY{n}{initial\PYZus{}topics}\PY{o}{=}\PY{n}{m}\PY{o}{.}\PY{n}{topics}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Initializing from provided topics and vocabulary.
    \end{verbatim}

    
    
    \begin{verbatim}
Learning a topic model
    \end{verbatim}

    
    
    \begin{verbatim}
       Number of documents     72269
    \end{verbatim}

    
    
    \begin{verbatim}
           Vocabulary size    171005
    \end{verbatim}

    
    
    \begin{verbatim}
   Running collapsed Gibbs sampling
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| Iteration | Elapsed Time  | Tokens/Second  | Est. Perplexity |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| 10        | 2.99s         | 7.251e+06      | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    \hypertarget{seeding-the-model-with-prior-knowledge}{%
\subsubsection{Seeding the model with prior
knowledge}\label{seeding-the-model-with-prior-knowledge}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{associations} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{SFrame}\PY{p}{(}\PY{p}{)}
         \PY{n}{associations}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{recognition}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{associations}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{topic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{m2} \PY{o}{=} \PY{n}{tc}\PY{o}{.}\PY{n}{topic\PYZus{}model}\PY{o}{.}\PY{n}{create}\PY{p}{(}\PY{n}{docs}\PY{p}{,}
                                          \PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
                                          \PY{n}{num\PYZus{}iterations}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,}
                                          \PY{n}{associations}\PY{o}{=}\PY{n}{associations}\PY{p}{,} 
                                          \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Learning a topic model
    \end{verbatim}

    
    
    \begin{verbatim}
       Number of documents     72269
    \end{verbatim}

    
    
    \begin{verbatim}
           Vocabulary size    171005
    \end{verbatim}

    
    
    \begin{verbatim}
   Running collapsed Gibbs sampling
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| Iteration | Elapsed Time  | Tokens/Second  | Est. Perplexity |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    
    \begin{verbatim}
| 10        | 2.10s         | 1.04058e+07    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 20        | 3.97s         | 1.09325e+07    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 30        | 5.79s         | 9.42067e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 40        | 7.66s         | 1.0637e+07     | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
| 50        | 9.51s         | 9.86708e+06    | 0               |
    \end{verbatim}

    
    
    \begin{verbatim}
+-----------+---------------+----------------+-----------------+
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{m2}\PY{o}{.}\PY{n}{get\PYZus{}topics}\PY{p}{(}\PY{n}{num\PYZus{}words}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}54}]:} Columns:
         	topic	int
         	word	str
         	score	float
         
         Rows: 200
         
         Data:
         +-------+----------+----------------------+
         | topic |   word   |        score         |
         +-------+----------+----------------------+
         |   0   |   line   |  0.0109206038501584  |
         |   0   |  german  | 0.010542910113799127 |
         |   0   |    de    | 0.010148794910641626 |
         |   0   | railway  | 0.010079824750089063 |
         |   0   | english  | 0.009242329943379372 |
         |   0   | chinese  | 0.008900763433976205 |
         |   0   | language | 0.008654441432002766 |
         |   0   |  china   | 0.008490226764020474 |
         |   0   |  large   | 0.008004151346792889 |
         |   0   | russian  | 0.007705280651065117 |
         +-------+----------+----------------------+
         [200 rows x 3 columns]
         Note: Only the head of the SFrame is printed.
         You can use print\_rows(num\_rows=m, num\_columns=n) to print more rows and columns.
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{n}{print\PYZus{}topics}\PY{p}{(}\PY{n}{m2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['german', 'line', 'english', 'de', 'railway']
['son', 'time', 'book', 'life', 'john']
['role', 'episode', 'show', 'film', 'series']
['york', 'station', 'park', 'de', 'company']
['information', 'law', 'time', 'work', 'social']
['west', 'county', 'north', 'city', 'east']
['years', '18', 'population', 'town', 'age']
['games', 'team', 'game', 'won', 'season']
['company', 'aircraft', 'air', 'force', 'division']
['india', 'art', 'century', 'roman', 'church']
['government', 'students', 'national', 'state', 'party']
['schools', 'college', 'university', 'school', 'high']
['series', 'world', 'back', 'king', 'time']
['services', 'system', 'service', 'million', 'engine']
['area', 'built', 'river', 'road', 'region']
['systems', 'set', 'number', 'system', 'data']
['water', 'small', 'species', 'food', 'found']
['league', 'year', 'club', 'song', 'time']
['released', 'music', 'songs', 'album', 'band']
['army', 'united', 'states', 'court', 'war']

    \end{Verbatim}

    \hypertarget{ux9605ux8bfbux6750ux6599}{%
\section{阅读材料}\label{ux9605ux8bfbux6750ux6599}}

https://dato.com/learn/userguide/text/topic-models.html


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
